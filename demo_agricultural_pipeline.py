#!/usr/bin/env python3
"""
Script de d√©monstration du pipeline agricole DataEnginerFlow360
Charge les donn√©es agricoles et les envoie vers Kafka pour traitement en temps r√©el
"""
import json
import time
from pathlib import Path
from datetime import datetime
import random

print("=" * 70)
print("üåæ D√âMONSTRATION DU PIPELINE AGRICOLE DATAENGINERFLOW360")
print("=" * 70)
print(f"‚è∞ Heure de d√©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# Chemins des donn√©es
DATA_DIR = Path("data_lake/raw/agriculture")

# ============================================================================
# √âTAPE 1: Chargement des donn√©es g√©n√©r√©es
# ============================================================================
print("üì• √âTAPE 1: Chargement des donn√©es agricoles")
print("-" * 70)

datasets = {}
for filename in ['farms', 'fields', 'crops', 'harvests', 'weather', 'sensors', 'sales']:
    filepath = DATA_DIR / f'{filename}.json'
    if filepath.exists():
        with open(filepath, 'r', encoding='utf-8') as f:
            datasets[filename] = json.load(f)
        print(f"   ‚úÖ {filename}: {len(datasets[filename])} enregistrements charg√©s")
    else:
        print(f"   ‚ö†Ô∏è  {filename}: fichier non trouv√©")
        datasets[filename] = []

print()

# ============================================================================
# √âTAPE 2: Statistiques des donn√©es
# ============================================================================
print("üìä √âTAPE 2: Statistiques des donn√©es agricoles")
print("-" * 70)

# Statistiques par type de culture
crops_by_type = {}
for crop in datasets['crops']:
    crop_type = crop['crop_type']
    crops_by_type[crop_type] = crops_by_type.get(crop_type, 0) + 1

print("   üìà R√©partition des cultures:")
for crop_type, count in sorted(crops_by_type.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"      - {crop_type}: {count} parcelles")

# Statistiques de r√©colte
total_harvest = sum(h['quantity_tonnes'] for h in datasets['harvests'])
total_revenue = sum(s['total_amount'] for s in datasets['sales'])

print(f"\n   üí∞ Production et revenus:")
print(f"      - Production totale: {total_harvest:.2f} tonnes")
print(f"      - Revenus totaux: {total_revenue:,.2f} FCFA")
print(f"      - Prix moyen: {total_revenue/total_harvest:.2f} FCFA/tonne")

# Statistiques capteurs
sensors_by_type = {}
for sensor in datasets['sensors']:
    sensor_type = sensor['sensor_type']
    sensors_by_type[sensor_type] = sensors_by_type.get(sensor_type, 0) + 1

print(f"\n   üî¨ Capteurs IoT:")
for sensor_type, count in sensors_by_type.items():
    print(f"      - {sensor_type}: {count} lectures")

print()

# ============================================================================
# √âTAPE 3: Envoi des donn√©es capteurs vers Kafka
# ============================================================================
print("üì® √âTAPE 3: Envoi des donn√©es vers Kafka")
print("-" * 70)

try:
    from kafka import KafkaProducer
    from kafka.admin import KafkaAdminClient, NewTopic
    
    # Cr√©er les topics
    admin_client = KafkaAdminClient(
        bootstrap_servers='localhost:9093',
        client_id='agricultural_demo'
    )
    
    topics_to_create = [
        'sensor_data',
        'weather_updates',
        'harvest_events',
    ]
    
    existing_topics = admin_client.list_topics()
    
    for topic_name in topics_to_create:
        if topic_name not in existing_topics:
            topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)
            admin_client.create_topics(new_topics=[topic], validate_only=False)
            print(f"   ‚úÖ Topic cr√©√©: {topic_name}")
        else:
            print(f"   ‚úÖ Topic existant: {topic_name}")
    
    # Cr√©er le producteur
    producer = KafkaProducer(
        bootstrap_servers='localhost:9093',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    # Envoyer des donn√©es capteurs
    num_sensor_messages = min(50, len(datasets['sensors']))
    print(f"\n   üì§ Envoi de {num_sensor_messages} lectures de capteurs...")
    for sensor_data in random.sample(datasets['sensors'], num_sensor_messages):
        producer.send('sensor_data', value=sensor_data)
    
    # Envoyer des donn√©es m√©t√©o
    num_weather_messages = min(20, len(datasets['weather']))
    print(f"   üì§ Envoi de {num_weather_messages} donn√©es m√©t√©o...")
    for weather_data in random.sample(datasets['weather'], num_weather_messages):
        producer.send('weather_updates', value=weather_data)
    
    # Envoyer des √©v√©nements de r√©colte
    num_harvest_messages = min(30, len(datasets['harvests']))
    print(f"   üì§ Envoi de {num_harvest_messages} √©v√©nements de r√©colte...")
    for harvest_data in random.sample(datasets['harvests'], num_harvest_messages):
        producer.send('harvest_events', value=harvest_data)
    
    producer.flush()
    producer.close()
    
    print(f"\n   ‚úÖ Total: {num_sensor_messages + num_weather_messages + num_harvest_messages} messages envoy√©s √† Kafka")
    print()
    
except Exception as e:
    print(f"   ‚ö†Ô∏è  Erreur Kafka: {e}")
    print(f"   ‚ÑπÔ∏è  Assurez-vous que Kafka est d√©marr√©")
    print()

# ============================================================================
# √âTAPE 4: V√©rification PostgreSQL
# ============================================================================
print("üóÑÔ∏è  √âTAPE 4: V√©rification de PostgreSQL")
print("-" * 70)

try:
    import psycopg2
    
    conn = psycopg2.connect(
        host='localhost',
        port=5433,
        database='dataenginerflow360',
        user='postgres',
        password='postgres'
    )
    cursor = conn.cursor()
    
    # Cr√©er un sch√©ma pour l'agriculture si n√©cessaire
    cursor.execute("CREATE SCHEMA IF NOT EXISTS agriculture")
    conn.commit()
    
    print(f"   ‚úÖ Sch√©ma 'agriculture' cr√©√©/v√©rifi√©")
    
    # V√©rifier les sch√©mas disponibles
    cursor.execute("""
        SELECT schema_name 
        FROM information_schema.schemata 
        WHERE schema_name NOT IN ('pg_catalog', 'information_schema')
        ORDER BY schema_name
    """)
    schemas = cursor.fetchall()
    print(f"   ‚úÖ Sch√©mas disponibles: {[s[0] for s in schemas]}")
    
    cursor.close()
    conn.close()
    print()
    
except Exception as e:
    print(f"   ‚ö†Ô∏è  Erreur PostgreSQL: {e}")
    print()

# ============================================================================
# √âTAPE 5: Exemples de donn√©es
# ============================================================================
print("üìã √âTAPE 5: Exemples de donn√©es g√©n√©r√©es")
print("-" * 70)

if datasets['farms']:
    print("\n   üè° Exemple d'exploitation:")
    farm = datasets['farms'][0]
    print(f"      ID: {farm['farm_id']}")
    print(f"      Nom: {farm['name']}")
    print(f"      Propri√©taire: {farm['owner']}")
    print(f"      R√©gion: {farm['location']['region']}")
    print(f"      Surface: {farm['total_area_ha']} ha")
    print(f"      Type: {farm['farming_type']}")

if datasets['crops']:
    print("\n   üå± Exemple de culture:")
    crop = datasets['crops'][0]
    print(f"      ID: {crop['crop_id']}")
    print(f"      Type: {crop['crop_type']} ({crop['category']})")
    print(f"      Vari√©t√©: {crop['variety']}")
    print(f"      Date de semis: {crop['sowing_date']}")
    print(f"      Rendement estim√©: {crop['estimated_yield_t_ha']} t/ha")

if datasets['sensors']:
    print("\n   üî¨ Exemple de lecture capteur:")
    sensor = datasets['sensors'][0]
    print(f"      ID: {sensor['sensor_id']}")
    print(f"      Type: {sensor['sensor_type']}")
    print(f"      Valeur: {sensor['value']} {sensor['unit']}")
    print(f"      Timestamp: {sensor['timestamp']}")

print()

# ============================================================================
# √âTAPE 6: V√©rification des services
# ============================================================================
print("üîç √âTAPE 6: V√©rification des services du pipeline")
print("-" * 70)

try:
    import requests
    
    # Airflow
    try:
        response = requests.get('http://localhost:8080/health', timeout=3)
        if response.status_code == 200:
            print(f"   ‚úÖ Airflow: Op√©rationnel")
        else:
            print(f"   ‚ö†Ô∏è  Airflow: Code {response.status_code}")
    except:
        print(f"   ‚ö†Ô∏è  Airflow: Non accessible")
    
    # Prometheus
    try:
        response = requests.get('http://localhost:9090/api/v1/targets', timeout=3)
        if response.status_code == 200:
            data = response.json()
            active = len(data['data']['activeTargets'])
            print(f"   ‚úÖ Prometheus: {active} targets actifs")
        else:
            print(f"   ‚ö†Ô∏è  Prometheus: Code {response.status_code}")
    except:
        print(f"   ‚ö†Ô∏è  Prometheus: Non accessible")
    
    # Grafana
    try:
        response = requests.get('http://localhost:3000/api/health', timeout=3)
        if response.status_code == 200:
            print(f"   ‚úÖ Grafana: Op√©rationnel")
        else:
            print(f"   ‚ö†Ô∏è  Grafana: Code {response.status_code}")
    except:
        print(f"   ‚ö†Ô∏è  Grafana: Non accessible")
    
    print()
    
except Exception as e:
    print(f"   ‚ö†Ô∏è  Erreur lors de la v√©rification: {e}")
    print()

# ============================================================================
# R√âSUM√â
# ============================================================================
print("=" * 70)
print("‚úÖ D√âMONSTRATION AGRICOLE TERMIN√âE")
print("=" * 70)
print()
print("üìä R√©sum√© de la d√©monstration:")
print(f"   ‚úÖ Exploitations: {len(datasets['farms'])}")
print(f"   ‚úÖ Parcelles: {len(datasets['fields'])}")
print(f"   ‚úÖ Cultures: {len(datasets['crops'])}")
print(f"   ‚úÖ R√©coltes: {len(datasets['harvests'])}")
print(f"   ‚úÖ Donn√©es m√©t√©o: {len(datasets['weather'])} jours")
print(f"   ‚úÖ Lectures capteurs: {len(datasets['sensors'])}")
print(f"   ‚úÖ Ventes: {len(datasets['sales'])}")
print()
print("üåê Prochaines √©tapes:")
print("   1. Consulter Airflow: http://localhost:8080 (admin/admin)")
print("   2. Visualiser dans Grafana: http://localhost:3000 (admin/admin)")
print("   3. V√©rifier les m√©triques: http://localhost:9090")
print("   4. Consommer les messages Kafka:")
print("      docker exec dataflow_kafka kafka-console-consumer \\")
print("        --bootstrap-server localhost:9092 \\")
print("        --topic sensor_data --from-beginning --max-messages 5")
print()
print(f"‚è∞ Heure de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 70)
