# DataEnginerFlow360 ğŸš€

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/pyspark-3.5.0-orange.svg)](https://spark.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-1.10.15-FF694B.svg)](https://www.getdbt.com/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)
[![Airflow](https://img.shields.io/badge/airflow-orchestration-017CEE.svg)](https://airflow.apache.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-production%20ready-success.svg)]()

Pipeline de donnÃ©es end-to-end avec ingestion, transformation, orchestration et monitoring.

## ğŸ“Š Architecture

```
Ingestion â†’ Processing â†’ Orchestration â†’ Monitoring
   â†“           â†“            â†“              â†“
 Kafka     PySpark       Airflow      Prometheus
           dbt                        Grafana
```

## âœ¨ FonctionnalitÃ©s

- **Ingestion**: Batch (API, fichiers) et Streaming (Kafka)
- **Processing**:
  - PySpark Structured Streaming (temps rÃ©el)
  - PySpark Batch (transformations complexes)
  - dbt (Data Warehouse avec star schema)
- **Orchestration**: 4 DAGs Airflow avec dÃ©pendances
- **Monitoring**: Prometheus + Grafana + Exporters
- **Tests**: Tests de qualitÃ© dbt (7/7 PASS)

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Docker & Docker Compose
- Python 3.12+
- Java 11 (pour PySpark)

### Installation

```bash
# Cloner le repo
git clone https://github.com/VOTRE_USERNAME/DataEnginerFlow360.git
cd DataEnginerFlow360

# CrÃ©er l'environnement virtuel
python3 -m venv venv
source venv/bin/activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# DÃ©marrer les services
cd docker
docker-compose up -d
```

### Test du Pipeline

```bash
# Tester dbt
cd transformation/dbt
dbt run
dbt test

# Tester le pipeline complet
python3 test_pipeline.py
```

## ğŸ“ Structure du Projet

```
DataEnginerFlow360/
â”œâ”€â”€ ingestion/              # Scripts d'ingestion
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â””â”€â”€ sources/
â”œâ”€â”€ transformation/         # Transformations
â”‚   â”œâ”€â”€ streaming_processing.py  # PySpark Streaming
â”‚   â”œâ”€â”€ spark_transformation.py  # PySpark Batch
â”‚   â””â”€â”€ dbt/                     # ModÃ¨les dbt
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ marts/
â”‚       â”‚       â”œâ”€â”€ dim_users.sql
â”‚       â”‚       â””â”€â”€ fact_transactions.sql
â”‚       â””â”€â”€ profiles.yml
â”œâ”€â”€ orchestration/dags/     # DAGs Airflow
â”‚   â”œâ”€â”€ batch_ingestion_dag.py
â”‚   â”œâ”€â”€ pyspark_transformation_dag.py
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â””â”€â”€ master_pipeline_dag.py
â”œâ”€â”€ monitoring/             # Monitoring
â”‚   â””â”€â”€ metrics_collector.py
â”œâ”€â”€ docker/                 # Configuration Docker
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”‚   â””â”€â”€ alert_rules.yml
â”‚   â””â”€â”€ statsd_mapping.yml
â””â”€â”€ data_lake/             # Stockage des donnÃ©es
    â”œâ”€â”€ raw/
    â”œâ”€â”€ processed/
    â””â”€â”€ curated/
```

## ğŸŒ Interfaces

| Service    | URL                   | Credentials |
| ---------- | --------------------- | ----------- |
| Grafana    | http://localhost:3000 | admin/admin |
| Prometheus | http://localhost:9090 | -           |
| Airflow    | http://localhost:8080 | admin/admin |
| cAdvisor   | http://localhost:8082 | -           |

## ğŸ“Š Dashboards Grafana

1. **Services Monitoring**: Ã‰tat des services, connexions PostgreSQL
2. **Data Quality**: MÃ©triques de qualitÃ©, tests dbt
3. **Infrastructure**: CPU, mÃ©moire, disque des conteneurs

## ğŸ”§ Configuration

### PostgreSQL

- Host: localhost
- Port: 5432
- Database: dataenginerflow360
- User: postgres
- Password: 1234

### Kafka

- Bootstrap: localhost:9093

### Prometheus

- Scrape interval: 15s
- Targets: postgres-exporter, statsd-exporter, kafka-exporter, cadvisor

## ğŸ“ˆ MÃ©triques CollectÃ©es

- **Airflow**: DAG duration, success/failure, task metrics
- **PostgreSQL**: Connexions, transactions, performance
- **Kafka**: Topics, consumer lag
- **Docker**: CPU, mÃ©moire, rÃ©seau
- **Data Quality**: Lignes traitÃ©es, erreurs, fraÃ®cheur

## ğŸ§ª Tests

```bash
# Tests dbt
cd transformation/dbt
dbt test

# Test du pipeline complet
python3 test_pipeline.py
```

## ğŸ“ DAGs Airflow

1. **batch_ingestion**: Ingestion quotidienne (2:00 AM)
2. **pyspark_transformation**: Transformations PySpark (3:00 AM)
3. **dbt_transformation**: Transformations dbt (4:00 AM)
4. **master_data_pipeline**: Orchestration globale

## ğŸ› ï¸ Technologies

- **Ingestion**: Kafka, Python, Faker
- **Processing**: PySpark 3.5.0, dbt 1.10.15
- **Orchestration**: Apache Airflow
- **Storage**: PostgreSQL, Data Lake (Parquet)
- **Monitoring**: Prometheus, Grafana
- **Containerization**: Docker, Docker Compose

## ğŸ“š Documentation

- [Walkthrough](docs/walkthrough.md)
- [Deployment Report](docs/deployment_report.md)
- [Implementation Plan](docs/implementation_plan.md)

## ğŸ¤ Contribution

Les contributions sont les bienvenues! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## ğŸ“„ Licence

MIT License

## ğŸ‘¤ Auteur

Mariama - [GitHub](https://github.com/VOTRE_USERNAME)

## ğŸ¯ Statut du Projet

âœ… **Production Ready** - Tous les composants sont testÃ©s et opÃ©rationnels

---

**â­ Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile!**
